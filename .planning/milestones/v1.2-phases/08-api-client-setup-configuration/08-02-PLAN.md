---
phase: 08-api-client-setup-configuration
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/OpenAnima.Core/LLM/LLMService.cs
  - src/OpenAnima.Core/Program.cs
autonomous: true
requirements: [LLM-01, LLM-02, LLM-03, LLM-04, LLM-05]

must_haves:
  truths:
    - "Runtime can receive streaming responses token-by-token from LLM API"
    - "ChatClient is registered as singleton and ILLMService is available via DI"
    - "SignalR circuit timeout is configured to 60+ seconds for long-running LLM calls"
    - "LLM configuration is bound from appsettings.json with validation on startup"
  artifacts:
    - path: "src/OpenAnima.Core/LLM/LLMService.cs"
      provides: "Streaming implementation via IAsyncEnumerable"
      contains: "CompleteChatStreamingAsync"
    - path: "src/OpenAnima.Core/Program.cs"
      provides: "DI registration for ChatClient, LLMOptions, ILLMService, SignalR config"
      contains: "ChatClient"
  key_links:
    - from: "src/OpenAnima.Core/Program.cs"
      to: "src/OpenAnima.Core/LLM/LLMOptions.cs"
      via: "IOptions binding from appsettings.json LLM section"
      pattern: "Configure<LLMOptions>"
    - from: "src/OpenAnima.Core/Program.cs"
      to: "OpenAI.Chat.ChatClient"
      via: "Singleton registration with LLMOptions values"
      pattern: "AddSingleton<ChatClient>"
    - from: "src/OpenAnima.Core/Program.cs"
      to: "src/OpenAnima.Core/LLM/LLMService.cs"
      via: "DI registration as ILLMService"
      pattern: "AddSingleton<ILLMService, LLMService>"
    - from: "src/OpenAnima.Core/Program.cs"
      to: "SignalR configuration"
      via: "Circuit timeout and keepalive settings"
      pattern: "AddCircuitOptions|ClientTimeoutInterval"
---

<objective>
Wire up streaming support, DI registration, and SignalR timeout configuration.

Purpose: Complete the LLM integration layer so it's fully operational — streaming works, services are registered in DI, and SignalR won't disconnect during long LLM calls. After this plan, Phase 9 (Chat UI) can inject ILLMService and call both CompleteAsync and StreamAsync.
Output: Fully wired LLM service with streaming, registered in DI, with SignalR configured for long-running operations.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-api-client-setup-configuration/8-RESEARCH.md
@.planning/phases/08-api-client-setup-configuration/08-01-SUMMARY.md
@src/OpenAnima.Core/Program.cs
@src/OpenAnima.Core/LLM/LLMOptions.cs
@src/OpenAnima.Core/LLM/ILLMService.cs
@src/OpenAnima.Core/LLM/LLMService.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement streaming in LLMService</name>
  <files>
    src/OpenAnima.Core/LLM/LLMService.cs
  </files>
  <action>
Replace the `StreamAsync` stub (NotImplementedException) with a real implementation:

1. Implement `StreamAsync` as `async IAsyncEnumerable<string>`:
   - Map `ChatMessageInput` list to OpenAI SDK `ChatMessage` list (reuse same mapping logic from CompleteAsync — extract a private helper method `MapMessages`)
   - Call `_client.CompleteChatStreamingAsync(messages)` to get the streaming enumerable
   - Iterate with `await foreach (StreamingChatCompletionUpdate update in ...)`
   - For each update: check `update.ContentUpdate.Count > 0`, then yield `update.ContentUpdate[0].Text`
   - Wrap the entire streaming loop in try/catch:
     - `ClientResultException` → yield a single error token like `"\n\n[Error: {mapped message}]"` then yield break
     - `HttpRequestException` → yield `"\n\n[Error: Network error - {message}]"` then yield break
     - `TaskCanceledException` → just yield break (cancellation is normal)
   - Add `[EnumeratorCancellation]` attribute on the CancellationToken parameter
   - Log errors with `_logger.LogError` before yielding error tokens

2. Extract private helper `MapMessages(IReadOnlyList<ChatMessageInput> inputs)` that returns `List<ChatMessage>`:
   - "system" → `new SystemChatMessage(input.Content)`
   - "user" → `new UserChatMessage(input.Content)`
   - "assistant" → `new AssistantChatMessage(input.Content)`
   - Default → `new UserChatMessage(input.Content)` (safe fallback)
   - Use this helper in both `CompleteAsync` and `StreamAsync`

NOTE: Do NOT use InvokeAsync here — that's a Blazor component concern. LLMService is a plain service. The Blazor component (Phase 9) will handle InvokeAsync when consuming the stream.
  </action>
  <verify>
    - `dotnet build src/OpenAnima.Core` compiles without errors
    - `LLMService.cs` contains `CompleteChatStreamingAsync` call
    - `LLMService.cs` has `MapMessages` private helper used by both methods
    - `StreamAsync` no longer throws `NotImplementedException`
    - `StreamAsync` has error handling that yields error tokens instead of throwing
  </verify>
  <done>
    StreamAsync yields tokens from CompleteChatStreamingAsync. Error handling yields inline error messages. MapMessages helper is shared between CompleteAsync and StreamAsync.
  </done>
</task>

<task type="auto">
  <name>Task 2: Register LLM services in DI and configure SignalR timeouts</name>
  <files>
    src/OpenAnima.Core/Program.cs
  </files>
  <action>
Update `src/OpenAnima.Core/Program.cs` to wire up all LLM services and configure SignalR for long-running operations.

1. Add using statements at top:
   ```
   using OpenAnima.Core.LLM;
   using OpenAI;
   using OpenAI.Chat;
   using Microsoft.Extensions.Options;
   ```

2. After the existing service facade registrations (after `AddSingleton<IEventBusService, EventBusService>()`), add LLM service registration block:

   a. Bind and validate LLMOptions:
      ```csharp
      builder.Services.AddOptions<LLMOptions>()
          .Bind(builder.Configuration.GetSection(LLMOptions.SectionName))
          .ValidateDataAnnotations()
          .ValidateOnStart();
      ```

   b. Register ChatClient as singleton:
      ```csharp
      builder.Services.AddSingleton<ChatClient>(sp =>
      {
          var options = sp.GetRequiredService<IOptions<LLMOptions>>().Value;
          var clientOptions = new OpenAIClientOptions
          {
              Endpoint = new Uri(options.Endpoint)
          };
          return new ChatClient(
              model: options.Model,
              credential: new ApiKeyCredential(options.ApiKey),
              options: clientOptions);
      });
      ```

   c. Register ILLMService:
      ```csharp
      builder.Services.AddSingleton<ILLMService, LLMService>();
      ```

3. Update the existing SignalR registration. Replace `builder.Services.AddSignalR();` with:
   ```csharp
   builder.Services.AddSignalR(options =>
   {
       options.ClientTimeoutInterval = TimeSpan.FromSeconds(60);
       options.HandshakeTimeout = TimeSpan.FromSeconds(30);
       options.KeepAliveInterval = TimeSpan.FromSeconds(15);
   });
   ```

4. Add Blazor circuit options for long-running operations. After `AddInteractiveServerComponents()`, add:
   ```csharp
   builder.Services.AddServerSideBlazor()
       .AddCircuitOptions(options =>
       {
           options.DisconnectedCircuitMaxRetained = 100;
           options.DisconnectedCircuitRetentionPeriod = TimeSpan.FromMinutes(3);
       });
   ```

IMPORTANT: Keep all existing registrations intact. Only ADD new lines and MODIFY the SignalR line. Do not remove or reorder existing code.
  </action>
  <verify>
    - `dotnet build src/OpenAnima.Core` compiles without errors
    - Program.cs contains `Configure<LLMOptions>` or `AddOptions<LLMOptions>`
    - Program.cs contains `AddSingleton<ChatClient>`
    - Program.cs contains `AddSingleton<ILLMService, LLMService>`
    - Program.cs contains `ClientTimeoutInterval` set to 60 seconds
    - Program.cs contains `DisconnectedCircuitRetentionPeriod` set to 3 minutes
    - All existing service registrations (PluginRegistry, EventBus, HeartbeatLoop, etc.) still present
  </verify>
  <done>
    LLMOptions bound from appsettings.json with validation. ChatClient registered as singleton. ILLMService registered. SignalR configured with 60s client timeout and 3-minute circuit retention. All existing services preserved.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build src/OpenAnima.Core` — project compiles cleanly
2. Program.cs has LLMOptions binding, ChatClient singleton, ILLMService registration
3. SignalR has ClientTimeoutInterval >= 60s, KeepAliveInterval = 15s
4. Blazor circuit has DisconnectedCircuitRetentionPeriod >= 3 minutes
5. LLMService.StreamAsync yields tokens via CompleteChatStreamingAsync
6. LLMService.StreamAsync handles errors gracefully (yields error tokens, doesn't throw)
7. All existing functionality (modules, heartbeat, SignalR hub) still works
</verification>

<success_criteria>
- Streaming implementation yields tokens from OpenAI SDK
- All LLM services registered in DI container
- SignalR configured for long-running LLM operations
- Project compiles and all existing functionality preserved
- Phase 9 can inject ILLMService and use both CompleteAsync and StreamAsync
</success_criteria>

<output>
After completion, create `.planning/phases/08-api-client-setup-configuration/08-02-SUMMARY.md`
</output>

---
phase: 08-api-client-setup-configuration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/OpenAnima.Core/OpenAnima.Core.csproj
  - src/OpenAnima.Core/LLM/LLMOptions.cs
  - src/OpenAnima.Core/LLM/ILLMService.cs
  - src/OpenAnima.Core/LLM/LLMService.cs
  - src/OpenAnima.Core/appsettings.json
autonomous: true
requirements: [LLM-01, LLM-02, LLM-04, LLM-05]

must_haves:
  truths:
    - "LLM endpoint, API key, and model are configurable via appsettings.json"
    - "Runtime can send chat messages and receive a complete LLM response"
    - "User sees meaningful error messages when API calls fail (401, 429, 500, network, timeout)"
    - "SDK automatically retries transient failures (408, 429, 500-504) with exponential backoff"
  artifacts:
    - path: "src/OpenAnima.Core/LLM/LLMOptions.cs"
      provides: "Type-safe LLM configuration model"
      contains: "class LLMOptions"
    - path: "src/OpenAnima.Core/LLM/ILLMService.cs"
      provides: "LLM service contract"
      exports: ["ILLMService"]
    - path: "src/OpenAnima.Core/LLM/LLMService.cs"
      provides: "ChatClient wrapper with error handling"
      contains: "class LLMService"
    - path: "src/OpenAnima.Core/appsettings.json"
      provides: "LLM configuration section"
      contains: "\"LLM\""
  key_links:
    - from: "src/OpenAnima.Core/LLM/LLMService.cs"
      to: "OpenAI.Chat.ChatClient"
      via: "ChatClient injected as singleton"
      pattern: "ChatClient"
    - from: "src/OpenAnima.Core/LLM/LLMService.cs"
      to: "src/OpenAnima.Core/LLM/LLMOptions.cs"
      via: "IOptions<LLMOptions> injection"
      pattern: "IOptions<LLMOptions>"
---

<objective>
Create the LLM API client foundation: configuration model, service interface, and implementation with error handling.

Purpose: Establish the core LLM integration layer that Phase 9 (Chat UI) will consume. This plan covers non-streaming chat completion, configuration, error mapping, and retry (via SDK built-in).
Output: Working LLMService that can call OpenAI-compatible APIs with proper error handling.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-api-client-setup-configuration/8-RESEARCH.md
@src/OpenAnima.Core/Program.cs
@src/OpenAnima.Core/OpenAnima.Core.csproj
@src/OpenAnima.Contracts/IEventBus.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM configuration and appsettings.json</name>
  <files>
    src/OpenAnima.Core/OpenAnima.Core.csproj
    src/OpenAnima.Core/LLM/LLMOptions.cs
    src/OpenAnima.Core/appsettings.json
  </files>
  <action>
1. Add OpenAI NuGet package to OpenAnima.Core.csproj:
   ```
   dotnet add src/OpenAnima.Core package OpenAI --version 2.8.0
   ```

2. Create `src/OpenAnima.Core/LLM/LLMOptions.cs`:
   - Namespace: `OpenAnima.Core.LLM`
   - Properties: `Endpoint` (string, default "https://api.openai.com/v1"), `ApiKey` (string, default empty), `Model` (string, default "gpt-4"), `MaxRetries` (int, default 3), `TimeoutSeconds` (int, default 120)
   - Add `public const string SectionName = "LLM";`
   - Add `[Required]` attribute on `ApiKey` for validation
   - Use `System.ComponentModel.DataAnnotations` for validation attributes

3. Create `src/OpenAnima.Core/appsettings.json`:
   - Add `LLM` section with Endpoint, ApiKey (empty placeholder), Model defaults
   - Add `Logging` section with default LogLevel Information
   - Add `AllowedHosts: "*"`
   - NOTE: appsettings.json is NOT gitignored, so use empty string for ApiKey (user fills in their key). Add a comment-style hint: the key name "ApiKey" is self-documenting.
  </action>
  <verify>
    - `dotnet build src/OpenAnima.Core` compiles without errors
    - `LLMOptions.cs` exists with all 5 properties and SectionName constant
    - `appsettings.json` exists with LLM section containing Endpoint, ApiKey, Model
  </verify>
  <done>
    LLMOptions model exists with typed properties and validation. appsettings.json has LLM configuration section. OpenAI 2.8.0 NuGet package is referenced.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ILLMService interface and LLMService implementation with error handling</name>
  <files>
    src/OpenAnima.Core/LLM/ILLMService.cs
    src/OpenAnima.Core/LLM/LLMService.cs
  </files>
  <action>
1. Create `src/OpenAnima.Core/LLM/ILLMService.cs`:
   - Namespace: `OpenAnima.Core.LLM`
   - Define `LLMResult` record: `bool Success`, `string? Content`, `string? Error`
   - Define `ILLMService` interface with:
     - `Task<LLMResult> CompleteAsync(IReadOnlyList<ChatMessageInput> messages, CancellationToken ct = default)` — non-streaming completion
     - `IAsyncEnumerable<string> StreamAsync(IReadOnlyList<ChatMessageInput> messages, CancellationToken ct = default)` — streaming (for Plan 02 to implement)
   - Define `ChatMessageInput` record: `string Role` ("system"/"user"/"assistant"), `string Content`
   - Use our own input types (not OpenAI SDK types) so the interface is SDK-agnostic

2. Create `src/OpenAnima.Core/LLM/LLMService.cs`:
   - Namespace: `OpenAnima.Core.LLM`
   - Constructor: inject `ChatClient` (singleton) and `ILogger<LLMService>`
   - Implement `CompleteAsync`:
     a. Map `ChatMessageInput` list to OpenAI SDK `ChatMessage` list: "system" → `SystemChatMessage`, "user" → `UserChatMessage`, "assistant" → `AssistantChatMessage`
     b. Call `_client.CompleteChatAsync(messages, ct)`
     c. Return `LLMResult { Success = true, Content = completion.Content[0].Text }`
     d. Wrap in try/catch with specific error handling:
        - `ClientResultException` with Status 401 → "Invalid API key. Check your LLM configuration."
        - `ClientResultException` with Status 429 → "Rate limit exceeded. Please wait and try again."
        - `ClientResultException` with Status >= 500 → "LLM service error. Please try again later."
        - `ClientResultException` with Status 404 → "Model not found. Check your model name in configuration."
        - `HttpRequestException` → "Network error: {message}"
        - `TaskCanceledException` → "Request timed out."
        - Generic `Exception` → "Unexpected error: {message}"
     e. Log all errors with `_logger.LogError`
   - Implement `StreamAsync` as a stub that throws `NotImplementedException("Streaming implemented in Plan 02")` — Plan 02 will fill this in
   - NOTE: The SDK automatically retries 408, 429, 500-504 up to 3 times with exponential backoff. Do NOT add custom retry logic. The catch blocks handle the FINAL failure after retries are exhausted.
  </action>
  <verify>
    - `dotnet build src/OpenAnima.Core` compiles without errors
    - `ILLMService.cs` defines `CompleteAsync` and `StreamAsync` methods
    - `LLMService.cs` has try/catch blocks for ClientResultException (401, 429, 404, 500+), HttpRequestException, TaskCanceledException
    - `LLMService.cs` does NOT contain any custom retry logic (SDK handles it)
  </verify>
  <done>
    ILLMService interface defines non-streaming and streaming contracts. LLMService implements non-streaming completion with comprehensive error handling mapping SDK exceptions to user-friendly messages. SDK built-in retry handles transient failures automatically.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build src/OpenAnima.Core` — project compiles with OpenAI 2.8.0 reference
2. All 4 new files exist: LLMOptions.cs, ILLMService.cs, LLMService.cs, appsettings.json
3. LLMOptions has SectionName, Endpoint, ApiKey, Model, MaxRetries, TimeoutSeconds
4. LLMService handles 5+ distinct error types with user-friendly messages
5. No custom retry logic exists (SDK built-in handles retries)
</verification>

<success_criteria>
- OpenAI 2.8.0 NuGet package added to project
- LLMOptions configuration model with validation
- appsettings.json with LLM section
- ILLMService interface with CompleteAsync and StreamAsync signatures
- LLMService with non-streaming completion and comprehensive error handling
- All code compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/08-api-client-setup-configuration/08-01-SUMMARY.md`
</output>

---
phase: 10-context-management-token-counting
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - src/OpenAnima.Core/Components/Shared/ChatPanel.razor
  - src/OpenAnima.Core/Components/Shared/ChatInput.razor
  - src/OpenAnima.Core/Components/Shared/ChatInput.razor.css
  - src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor
  - src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor.css
  - src/OpenAnima.Core/wwwroot/js/chat.js
autonomous: false
requirements: [CTX-01, CTX-02, CTX-03, CTX-04]

must_haves:
  truths:
    - "User can see cumulative token usage (input/output separately) near chat input"
    - "User can see current context capacity with color-coded status (green/yellow/red)"
    - "User is blocked from sending when context exceeds 90% with a modal warning"
    - "Token display updates after each message completes, not during streaming"
    - "Chat events are published to EventBus on message sent, response received, and context limit reached"
  artifacts:
    - path: "src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor"
      provides: "Token usage and context capacity display component"
      min_lines: 20
    - path: "src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor.css"
      provides: "Styling with color-coded context status"
    - path: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      provides: "Integrated context management, EventBus publishing, send blocking"
  key_links:
    - from: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      to: "src/OpenAnima.Core/Services/ChatContextManager.cs"
      via: "DI injection and method calls"
      pattern: "ChatContextManager"
    - from: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      to: "src/OpenAnima.Core/Events/ChatEvents.cs"
      via: "EventBus.PublishAsync with chat event payloads"
      pattern: "PublishAsync.*MessageSent|ResponseReceived|ContextLimitReached"
    - from: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      to: "src/OpenAnima.Core/LLM/ILLMService.cs"
      via: "StreamWithUsageAsync replacing StreamAsync"
      pattern: "StreamWithUsageAsync"
    - from: "src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor"
      to: "src/OpenAnima.Core/Services/ChatContextManager.cs"
      via: "Parameter binding from ChatPanel"
      pattern: "InputTokens|OutputTokens|CurrentContextTokens"
---

<objective>
Integrate context management into the chat UI: display token usage and context capacity near the chat input, block sends when approaching context limit with a modal, switch ChatPanel to use StreamWithUsageAsync for accurate token tracking, and publish chat events to EventBus.

Purpose: Give users visibility into token consumption and context capacity, prevent context overflow, and enable module integration via EventBus events.
Output: TokenUsageDisplay.razor component, updated ChatPanel.razor with context management, updated ChatInput.razor with send blocking, EventBus event publishing.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-context-management-token-counting/10-RESEARCH.md
@.planning/phases/10-context-management-token-counting/10-01-SUMMARY.md
@src/OpenAnima.Core/Components/Shared/ChatPanel.razor
@src/OpenAnima.Core/Components/Shared/ChatInput.razor
@src/OpenAnima.Core/Components/Shared/ChatInput.razor.css
@src/OpenAnima.Core/wwwroot/js/chat.js
@src/OpenAnima.Core/Services/ChatContextManager.cs
@src/OpenAnima.Core/Events/ChatEvents.cs
@src/OpenAnima.Core/LLM/ILLMService.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TokenUsageDisplay component and integrate context management into ChatPanel</name>
  <files>
    src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor
    src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor.css
    src/OpenAnima.Core/Components/Shared/ChatPanel.razor
    src/OpenAnima.Core/Components/Shared/ChatInput.razor
    src/OpenAnima.Core/Components/Shared/ChatInput.razor.css
    src/OpenAnima.Core/wwwroot/js/chat.js
  </files>
  <action>
1. Create `TokenUsageDisplay.razor` — a compact display component positioned near chat input (per user decision):
   - Two separate sections per user decision: Token Usage and Context Capacity
   - Token Usage section: shows "Input: {N} | Output: {N} | Total: {N}" — cumulative across all conversations
   - Context Capacity section: shows "{current} / {max} ({percentage}%)" with color-coded status
   - Parameters: `int InputTokens`, `int OutputTokens`, `int CurrentContextTokens`, `int MaxContextTokens`
   - Computed: `TotalTokens = InputTokens + OutputTokens`, `Percentage = CurrentContextTokens / MaxContextTokens * 100`
   - Color logic (Claude's discretion thresholds): Normal (green) < 70%, Warning (yellow) 70-85%, Danger (red) >= 85%
   - CSS classes: `status-normal` (green #4ade80), `status-warning` (yellow #fbbf24), `status-danger` (red #ef4444)
   - Layout: single row, compact, subtle text (small font, muted colors), positioned above the chat input area

2. Create `TokenUsageDisplay.razor.css`:
   - `.token-usage-display` — flex row, small font (0.75rem), muted color (#9ca3af), padding 0.25rem 1rem, gap between sections
   - `.token-section` and `.context-section` — inline display
   - `.status-normal` — color: #4ade80 (green)
   - `.status-warning` — color: #fbbf24 (yellow)
   - `.status-danger` — color: #ef4444 (red)
   - Keep it minimal and unobtrusive — this is informational, not primary UI

3. Update `ChatPanel.razor`:
   - Inject `ChatContextManager` and `IEventBus` (add `@inject ChatContextManager _contextManager` and `@inject IEventBus _eventBus`)
   - Add `using OpenAnima.Core.Events` and `using OpenAnima.Contracts`
   - Place `<TokenUsageDisplay>` component between the message list and `<ChatInput>`, passing parameters from ChatContextManager
   - Replace `_llmService.StreamAsync` with `_llmService.StreamWithUsageAsync` in both `SendMessage` and `RegenerateLastResponse` methods:
     - Each `StreamingResult` has `.Token`, `.InputTokens`, `.OutputTokens`
     - Accumulate content from `.Token` (same as before)
     - After streaming loop, check the final result's `.InputTokens` and `.OutputTokens` for API usage
   - In `SendMessage`:
     - BEFORE adding user message: call `_contextManager.CanSendMessage(history, userMessage)`. If false, show context limit modal via JS interop and publish ContextLimitReachedPayload event, then return without sending.
     - AFTER adding user message: call `_contextManager.UpdateAfterSend(messageTokenCount)` and publish MessageSentPayload event
     - AFTER streaming completes: call `_contextManager.UpdateAfterResponse(inputTokens, outputTokens, responseContextTokens)` and publish ResponseReceivedPayload event
     - Call `StateHasChanged` after context manager updates to refresh TokenUsageDisplay
   - In `RegenerateLastResponse`: apply same StreamWithUsageAsync and context manager updates
   - Subscribe to `_contextManager.OnStateChanged` in OnInitialized to trigger UI refresh
   - Add JS interop for context limit modal: `chatHelpers.showContextLimitModal(currentTokens, maxTokens)` — a simple confirm/alert dialog

4. Add `showContextLimitModal` function to `chat.js`:
   - Simple `alert()` or custom modal showing: "Context limit reached. Current: {current} tokens / Max: {max} tokens ({percentage}%). Please start a new conversation."
   - Returns void (blocking alert is fine since user decision says "弹窗提示并限制发送")

5. Update `ChatInput.razor`:
   - Add `[Parameter] public bool IsContextLimitReached { get; set; }` parameter
   - When `IsContextLimitReached` is true, disable the send button and textarea (same as IsDisabled behavior)
   - ChatPanel passes `IsContextLimitReached` from `_contextManager.GetContextStatus() == ContextStatus.Danger && utilization >= 90`

6. Update `ChatInput.razor.css`:
   - Add `.context-limit-warning` style for visual indication when context is full (optional subtle red border on textarea)
  </action>
  <verify>
    <automated>cd /home/user/OpenAnima && dotnet build src/OpenAnima.Core 2>&1 | tail -5</automated>
    <manual>Run the app, send messages, verify token display updates after each message, verify color changes as context fills, verify send blocking at threshold</manual>
  </verify>
  <done>TokenUsageDisplay shows cumulative input/output tokens and context capacity with color coding, ChatPanel uses StreamWithUsageAsync for accurate token tracking, sends are blocked at 90% threshold with modal, EventBus events published for message sent/response received/context limit reached, display updates after each message completion (not during streaming)</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify complete context management experience</name>
  <files>src/OpenAnima.Core/Components/Shared/TokenUsageDisplay.razor</files>
  <action>
    Human verification of the complete context management UI and behavior.
    What was built:
    - Token usage display (input/output/total) near chat input
    - Context capacity display with color-coded status (green/yellow/red)
    - Send blocking when context limit approached (90% threshold)
    - Accurate token tracking from API-returned usage
    - EventBus events for message sent, response received, context limit

    Steps to verify:
    1. Run `dotnet run --project src/OpenAnima.Core --no-browser` and open the dashboard
    2. Verify token usage display appears near chat input showing "Input: 0 | Output: 0 | Total: 0"
    3. Verify context capacity shows "0 / 128000 (0%)" in green
    4. Send a message — verify token counts update AFTER response completes (not during streaming)
    5. Verify input and output tokens are tracked separately
    6. Send several messages — verify cumulative totals increase
    7. Verify context capacity percentage increases with each exchange
    8. (Optional) Temporarily set MaxContextTokens to a small value (e.g., 500) in appsettings.json to test:
       - Yellow warning appears at ~70%
       - Red danger appears at ~85%
       - Send is blocked at ~90% with modal popup
  </action>
  <verify>Human confirms token display, color coding, and send blocking work correctly</verify>
  <done>User approves the complete context management experience — token tracking, color warnings, and send blocking all function as expected</done>
</task>

</tasks>

<verification>
1. `dotnet build src/OpenAnima.Core` — zero errors
2. TokenUsageDisplay.razor exists and renders token metrics
3. ChatPanel uses StreamWithUsageAsync and integrates ChatContextManager
4. Token display updates after message completion, not during streaming
5. Context capacity shows color-coded status (green < 70%, yellow 70-85%, red >= 85%)
6. Send blocked at 90% threshold with modal warning
7. Three EventBus events published: MessageSentPayload, ResponseReceivedPayload, ContextLimitReachedPayload
8. Human verification confirms working end-to-end experience
</verification>

<success_criteria>
- Token usage (input/output/total) visible near chat input
- Context capacity displayed separately with color coding
- Tokens update after each message completes (not during streaming)
- Cumulative totals track across conversation
- Send blocked with modal at 90% context threshold
- EventBus events fire on message sent, response received, and context limit
- Human approves the complete experience
</success_criteria>

<output>
After completion, create `.planning/phases/10-context-management-token-counting/10-02-SUMMARY.md`
</output>

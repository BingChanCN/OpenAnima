---
phase: 10-context-management-token-counting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/OpenAnima.Core/LLM/LLMOptions.cs
  - src/OpenAnima.Core/LLM/TokenCounter.cs
  - src/OpenAnima.Core/LLM/ILLMService.cs
  - src/OpenAnima.Core/LLM/LLMService.cs
  - src/OpenAnima.Core/Services/ChatContextManager.cs
  - src/OpenAnima.Core/Events/ChatEvents.cs
  - src/OpenAnima.Core/appsettings.json
  - src/OpenAnima.Core/Program.cs
  - src/OpenAnima.Core/OpenAnima.Core.csproj
autonomous: true
requirements: [CTX-01, CTX-02, CTX-04]

must_haves:
  truths:
    - "TokenCounter accurately counts tokens using SharpToken for any text input"
    - "ChatContextManager tracks cumulative input/output tokens and checks context thresholds"
    - "LLMService captures API-returned usage (input/output tokens) from streaming responses"
    - "ChatEvents payload types exist and can be published to EventBus"
    - "MaxContextTokens is configurable via appsettings.json LLM section"
  artifacts:
    - path: "src/OpenAnima.Core/LLM/TokenCounter.cs"
      provides: "SharpToken wrapper for model-aware token counting"
      exports: ["TokenCounter"]
    - path: "src/OpenAnima.Core/Services/ChatContextManager.cs"
      provides: "Context tracking, threshold checking, cumulative token accounting"
      exports: ["ChatContextManager", "ContextStatus"]
    - path: "src/OpenAnima.Core/Events/ChatEvents.cs"
      provides: "Event payload records for chat events"
      exports: ["MessageSentPayload", "ResponseReceivedPayload", "ContextLimitReachedPayload"]
    - path: "src/OpenAnima.Core/LLM/LLMService.cs"
      provides: "Streaming with usage capture via StreamOptions"
    - path: "src/OpenAnima.Core/LLM/LLMOptions.cs"
      provides: "MaxContextTokens configuration property"
  key_links:
    - from: "src/OpenAnima.Core/LLM/LLMService.cs"
      to: "StreamingChatCompletionUpdate.Usage"
      via: "StreamOptions.IncludeUsage = true"
      pattern: "IncludeUsage.*true"
    - from: "src/OpenAnima.Core/Services/ChatContextManager.cs"
      to: "src/OpenAnima.Core/LLM/TokenCounter.cs"
      via: "constructor injection"
      pattern: "TokenCounter"
    - from: "src/OpenAnima.Core/Events/ChatEvents.cs"
      to: "src/OpenAnima.Contracts/ModuleEvent.cs"
      via: "ModuleEvent<T> payload types"
      pattern: "record.*Payload"
---

<objective>
Build the backend services for context management and token counting: TokenCounter (SharpToken wrapper), ChatContextManager (threshold tracking), ChatEvents (EventBus payloads), and modify LLMService to capture API-returned usage from streaming responses.

Purpose: Provide the service layer that tracks token usage, enforces context limits, and defines chat event types — all consumed by the UI layer in Plan 02.
Output: TokenCounter.cs, ChatContextManager.cs, ChatEvents.cs, updated LLMService.cs with usage capture, updated LLMOptions.cs with MaxContextTokens.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-context-management-token-counting/10-RESEARCH.md
@.planning/phases/08-api-client-setup-configuration/08-01-SUMMARY.md
@.planning/phases/08-api-client-setup-configuration/08-02-SUMMARY.md
@src/OpenAnima.Core/LLM/LLMOptions.cs
@src/OpenAnima.Core/LLM/ILLMService.cs
@src/OpenAnima.Core/LLM/LLMService.cs
@src/OpenAnima.Core/Events/EventBus.cs
@src/OpenAnima.Contracts/IEventBus.cs
@src/OpenAnima.Contracts/ModuleEvent.cs
@src/OpenAnima.Core/Program.cs
@src/OpenAnima.Core/appsettings.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SharpToken package, create TokenCounter, ChatEvents, and update LLMOptions</name>
  <files>
    src/OpenAnima.Core/OpenAnima.Core.csproj
    src/OpenAnima.Core/LLM/LLMOptions.cs
    src/OpenAnima.Core/LLM/TokenCounter.cs
    src/OpenAnima.Core/Events/ChatEvents.cs
  </files>
  <action>
1. Install SharpToken NuGet package:
   `dotnet add src/OpenAnima.Core package SharpToken`

2. Add `MaxContextTokens` property to `LLMOptions.cs`:
   - Add `public int MaxContextTokens { get; set; } = 128000;` (default 128K for GPT-4 class models)
   - This is the model's context window size, configurable per user decision

3. Update `appsettings.json` to include `MaxContextTokens` in the LLM section:
   - Add `"MaxContextTokens": 128000` to the existing LLM config block

4. Create `src/OpenAnima.Core/LLM/TokenCounter.cs`:
   - Wrapper around SharpToken's `GptEncoding`
   - Constructor takes model name string, calls `GptEncoding.GetEncodingForModel(modelName)`. If model not recognized (e.g., "gpt-5-chat"), fall back to `GptEncoding.GetEncoding("cl100k_base")` with a try/catch.
   - `int CountTokens(string text)` — counts tokens in a single text string
   - `int CountMessages(IReadOnlyList<ChatMessageInput> messages)` — counts tokens for a conversation including per-message overhead (3 tokens per message for role/delimiters, plus 3 tokens for assistant reply priming). Uses the existing `ChatMessageInput` record from `ILLMService.cs`.
   - Register as singleton in DI (done in Task 2)

5. Create `src/OpenAnima.Core/Events/ChatEvents.cs`:
   - Three record types per user decision:
     - `record MessageSentPayload(string UserMessage, int TokenCount, DateTime Timestamp)` — published when user sends a message
     - `record ResponseReceivedPayload(string AssistantResponse, int InputTokens, int OutputTokens, DateTime Timestamp)` — published when LLM response completes, uses API-returned usage
     - `record ContextLimitReachedPayload(int CurrentTokens, int MaxTokens, double UtilizationPercentage)` — published when context threshold exceeded (per user decision: includes token counts and percentage)
   - Namespace: `OpenAnima.Core.Events`
   - These are plain records consumed by `ModuleEvent<T>` pattern from existing EventBus
  </action>
  <verify>
    <automated>cd /home/user/OpenAnima && dotnet build src/OpenAnima.Core --no-restore 2>&1 | tail -5</automated>
    <manual>Verify SharpToken is in csproj, LLMOptions has MaxContextTokens, TokenCounter.cs and ChatEvents.cs exist</manual>
  </verify>
  <done>SharpToken installed, TokenCounter counts tokens accurately with message overhead, ChatEvents defines three payload records, LLMOptions has MaxContextTokens property with 128000 default, appsettings.json updated</done>
</task>

<task type="auto">
  <name>Task 2: Create ChatContextManager, update LLMService streaming with usage capture, register DI</name>
  <files>
    src/OpenAnima.Core/Services/ChatContextManager.cs
    src/OpenAnima.Core/LLM/ILLMService.cs
    src/OpenAnima.Core/LLM/LLMService.cs
    src/OpenAnima.Core/Program.cs
  </files>
  <action>
1. Update `ILLMService.cs`:
   - Add a new record: `record StreamingResult(string Token, int? InputTokens, int? OutputTokens)` — wraps each streamed token with optional usage data (usage only appears in final chunk)
   - Add new method: `IAsyncEnumerable<StreamingResult> StreamWithUsageAsync(IReadOnlyList<ChatMessageInput> messages, CancellationToken ct = default)` — streams tokens AND captures API usage
   - Keep existing `StreamAsync` method unchanged for backward compatibility

2. Update `LLMService.cs`:
   - Implement `StreamWithUsageAsync`:
     - Create `ChatCompletionOptions` with `StreamOptions = new() { IncludeUsage = true }` to request usage in streaming response
     - Pass options to `_client.CompleteChatStreamingAsync(chatMessages, options, cancellationToken: ct)`
     - For each `StreamingChatCompletionUpdate`, check `update.Usage` — if not null, capture `InputTokenCount` and `OutputTokenCount`
     - Yield `new StreamingResult(text, null, null)` for content tokens
     - After streaming loop completes, yield a final `new StreamingResult("", inputTokens, outputTokens)` with the captured usage (if available)
     - Apply same error handling pattern as existing `StreamAsync` (yield error tokens, no exceptions)
   - Note: The OpenAI SDK 2.8.0 uses `update.Usage.InputTokenCount` and `update.Usage.OutputTokenCount` properties (NOT `InputTokens`/`OutputTokens`). Check the actual SDK property names — they may be `InputTokenCount`/`OutputTokenCount` or `InputTokens`/`OutputTokens`. Use whichever compiles.

3. Create `src/OpenAnima.Core/Services/ChatContextManager.cs`:
   - Namespace: `OpenAnima.Core.Services`
   - Inject `TokenCounter`, `IOptions<LLMOptions>`, `IEventBus`, `ILogger<ChatContextManager>`
   - Properties (all publicly readable):
     - `int TotalInputTokens` — cumulative input tokens across ALL conversations (per user decision)
     - `int TotalOutputTokens` — cumulative output tokens across ALL conversations
     - `int CurrentContextTokens` — tokens in current conversation context window
     - `int MaxContextTokens` — from LLMOptions config
   - Enum `ContextStatus { Normal, Warning, Danger }`
   - Thresholds (Claude's discretion): Warning at 70%, Danger at 85%, Block at 90%
   - Methods:
     - `bool CanSendMessage(IReadOnlyList<ChatMessageInput> currentHistory, string newMessage)` — uses TokenCounter to count projected total, returns false if >= 90% of MaxContextTokens
     - `ContextStatus GetContextStatus()` — returns status based on CurrentContextTokens vs MaxContextTokens
     - `void UpdateAfterSend(int messageTokens)` — adds to TotalInputTokens and CurrentContextTokens
     - `void UpdateAfterResponse(int inputTokens, int outputTokens, int responseContextTokens)` — updates TotalInputTokens/TotalOutputTokens with API-returned values, updates CurrentContextTokens
     - `double GetContextUtilization()` — returns CurrentContextTokens / MaxContextTokens as percentage (0-100)
     - `event Action? OnStateChanged` — fired after any update so UI can react
   - Thread-safe: use lock for token counter updates since Blazor Server is single-circuit but async

4. Register new services in `Program.cs`:
   - Register `TokenCounter` as singleton with factory: `new TokenCounter(sp.GetRequiredService<IOptions<LLMOptions>>().Value.Model)`
   - Register `ChatContextManager` as singleton
   - Place registrations after existing LLM service registrations
  </action>
  <verify>
    <automated>cd /home/user/OpenAnima && dotnet build src/OpenAnima.Core 2>&1 | tail -5</automated>
    <manual>Verify ChatContextManager has threshold logic, LLMService.StreamWithUsageAsync captures usage, DI registrations in Program.cs</manual>
  </verify>
  <done>ChatContextManager tracks cumulative and context tokens with threshold checking, LLMService captures API usage from streaming via StreamOptions, all services registered in DI, build succeeds</done>
</task>

</tasks>

<verification>
1. `dotnet build src/OpenAnima.Core` — zero errors
2. SharpToken package present in OpenAnima.Core.csproj
3. TokenCounter.cs exists with CountTokens and CountMessages methods
4. ChatContextManager.cs exists with CanSendMessage, GetContextStatus, threshold logic
5. ChatEvents.cs exists with three payload records
6. LLMService.cs has StreamWithUsageAsync with StreamOptions.IncludeUsage = true
7. LLMOptions.cs has MaxContextTokens property
8. Program.cs has TokenCounter and ChatContextManager DI registrations
</verification>

<success_criteria>
- Build passes with zero errors
- All backend services created and registered in DI
- Token counting uses SharpToken (not estimation)
- Streaming captures API-returned usage via StreamOptions
- Context manager enforces 90% block threshold
- Three chat event payload types defined for EventBus
- MaxContextTokens configurable via appsettings.json (default 128000)
</success_criteria>

<output>
After completion, create `.planning/phases/10-context-management-token-counting/10-01-SUMMARY.md`
</output>

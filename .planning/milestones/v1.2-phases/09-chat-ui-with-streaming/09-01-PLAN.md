---
phase: 09-chat-ui-with-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/OpenAnima.Core/OpenAnima.Core.csproj
  - src/OpenAnima.Core/wwwroot/js/chat.js
  - src/OpenAnima.Core/Components/App.razor
  - src/OpenAnima.Core/Components/Shared/ChatPanel.razor
  - src/OpenAnima.Core/Components/Shared/ChatPanel.razor.css
  - src/OpenAnima.Core/Components/Shared/ChatMessage.razor
  - src/OpenAnima.Core/Components/Shared/ChatMessage.razor.css
  - src/OpenAnima.Core/Components/Shared/ChatInput.razor
  - src/OpenAnima.Core/Components/Shared/ChatInput.razor.css
  - src/OpenAnima.Core/Components/Pages/Dashboard.razor
  - src/OpenAnima.Core/Components/Pages/Dashboard.razor.css
autonomous: true
requirements: [CHAT-01, CHAT-02, CHAT-03, CHAT-04]

must_haves:
  truths:
    - "User can type a message and send it from the chat panel"
    - "User sees conversation history with user messages right-aligned and assistant messages left-aligned"
    - "User sees LLM responses stream token-by-token in real time"
    - "Chat auto-scrolls to latest message unless user has scrolled up"
  artifacts:
    - path: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      provides: "Main chat container with conversation state and streaming logic"
      min_lines: 80
    - path: "src/OpenAnima.Core/Components/Shared/ChatInput.razor"
      provides: "Fixed-bottom input with auto-expand, Enter to send, Shift+Enter for newline"
      min_lines: 30
    - path: "src/OpenAnima.Core/Components/Shared/ChatMessage.razor"
      provides: "Individual message display with role-based styling"
      min_lines: 15
    - path: "src/OpenAnima.Core/wwwroot/js/chat.js"
      provides: "Auto-scroll detection, scroll-to-bottom, textarea auto-expand"
      min_lines: 20
  key_links:
    - from: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      to: "ILLMService"
      via: "DI injection, StreamAsync call"
      pattern: "ILLMService.*StreamAsync"
    - from: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      to: "src/OpenAnima.Core/wwwroot/js/chat.js"
      via: "IJSRuntime interop for auto-scroll"
      pattern: "chatHelpers\\.scrollToBottom"
    - from: "src/OpenAnima.Core/Components/Pages/Dashboard.razor"
      to: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      via: "Component embedding"
      pattern: "<ChatPanel"
---

<objective>
Build the core chat UI with streaming LLM responses integrated into the Dashboard.

Purpose: Enable users to have real-time conversations with the LLM agent, seeing tokens stream in as they arrive. This is the primary user-facing feature of v1.2.

Output: ChatPanel, ChatMessage, ChatInput components with streaming, auto-scroll JS helpers, Dashboard integration.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-api-client-setup-configuration/08-02-SUMMARY.md
@src/OpenAnima.Core/LLM/ILLMService.cs
@src/OpenAnima.Core/LLM/LLMService.cs
@src/OpenAnima.Core/Components/Pages/Dashboard.razor
@src/OpenAnima.Core/Components/Layout/MainLayout.razor
@src/OpenAnima.Core/Components/_Imports.razor
@src/OpenAnima.Core/Components/App.razor
@src/OpenAnima.Core/Program.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat infrastructure — JS helpers, NuGet packages, chat model</name>
  <files>
    src/OpenAnima.Core/OpenAnima.Core.csproj
    src/OpenAnima.Core/wwwroot/js/chat.js
    src/OpenAnima.Core/Components/App.razor
  </files>
  <action>
**1. Install Markdig packages** (needed for Plan 02 but install now to avoid csproj conflicts):
```bash
cd src/OpenAnima.Core
dotnet add package Markdig --version 0.37.0
dotnet add package Markdown.ColorCode --version 3.0.1
```

**2. Create `src/OpenAnima.Core/wwwroot/js/chat.js`** with these helpers:
```javascript
window.chatHelpers = {
    shouldAutoScroll: function(containerId) {
        const container = document.getElementById(containerId);
        if (!container) return false;
        const threshold = 100;
        return (container.scrollHeight - container.scrollTop - container.clientHeight) < threshold;
    },
    scrollToBottom: function(containerId) {
        const container = document.getElementById(containerId);
        if (!container) return;
        container.scrollTo({ top: container.scrollHeight, behavior: 'auto' });
    },
    autoExpand: function(textareaId) {
        const textarea = document.getElementById(textareaId);
        if (!textarea) return;
        textarea.style.height = 'auto';
        textarea.style.height = Math.min(textarea.scrollHeight, 200) + 'px';
    },
    resetTextarea: function(textareaId) {
        const textarea = document.getElementById(textareaId);
        if (!textarea) return;
        textarea.style.height = 'auto';
        textarea.value = '';
    },
    preventEnterNewline: function(textareaId) {
        // Called from Blazor to prevent default Enter behavior
        const textarea = document.getElementById(textareaId);
        if (!textarea) return;
        // Handled via event in Blazor side
    }
};
```

**3. Add script reference in `src/OpenAnima.Core/Components/App.razor`:**
Add `<script src="js/chat.js"></script>` BEFORE the `_framework/blazor.web.js` script tag in the body.

**4. Verify build:**
```bash
dotnet build src/OpenAnima.Core
```
  </action>
  <verify>
    <automated>cd /home/user/OpenAnima && dotnet build src/OpenAnima.Core --no-restore 2>&amp;1 | tail -5</automated>
    <manual>Check that chat.js exists in wwwroot/js/ and App.razor references it</manual>
  </verify>
  <done>Markdig packages installed, chat.js helpers created with auto-scroll/expand functions, App.razor references chat.js</done>
</task>

<task type="auto">
  <name>Task 2: Create ChatPanel, ChatMessage, ChatInput components with streaming and Dashboard integration</name>
  <files>
    src/OpenAnima.Core/Components/Shared/ChatPanel.razor
    src/OpenAnima.Core/Components/Shared/ChatPanel.razor.css
    src/OpenAnima.Core/Components/Shared/ChatMessage.razor
    src/OpenAnima.Core/Components/Shared/ChatMessage.razor.css
    src/OpenAnima.Core/Components/Shared/ChatInput.razor
    src/OpenAnima.Core/Components/Shared/ChatInput.razor.css
    src/OpenAnima.Core/Components/Pages/Dashboard.razor
    src/OpenAnima.Core/Components/Pages/Dashboard.razor.css
  </files>
  <action>
**ChatMessage.razor** — Individual message display component:
- Accept parameters: `string Role`, `string Content`, `bool IsStreaming`
- User messages: right-aligned, slightly different background (e.g., `rgba(99, 102, 241, 0.15)`)
- Assistant messages: left-aligned, different background (e.g., `rgba(255, 255, 255, 0.05)`), show AI icon (◆) on left side
- No timestamps per user decision
- Display content as plain text for now (Markdown rendering added in Plan 02)
- If `IsStreaming` is true, show a blinking cursor indicator (CSS animation) after content

**ChatMessage.razor.css** — Scoped styles:
- `.message` container with flex layout
- `.message.user` right-aligned with user background color
- `.message.assistant` left-aligned with assistant background color
- `.ai-icon` for the diamond icon on assistant messages
- `.message-content` with `max-width: 80%`, word-wrap, padding
- `.typing-cursor` blinking animation for streaming state
- Use existing dark theme variables (background: `#1a1b2e` area, text: light colors)

**ChatInput.razor** — Fixed-bottom input component:
- Accept `EventCallback<string> OnSend` parameter and `bool IsDisabled` parameter
- Textarea with `id="chat-input"`, `rows="1"`, placeholder "Type a message..."
- `@onkeydown` handler: if Enter without Shift → prevent default via JS interop, call OnSend with current text, clear input and reset height. If Shift+Enter → allow default (newline)
- `@oninput` handler: call `chatHelpers.autoExpand("chat-input")` via JS interop
- Send button (▶ or arrow icon) on the right side, disabled when input is empty or IsDisabled is true
- After sending, call `chatHelpers.resetTextarea("chat-input")` to reset height
- Use `@bind:event="oninput"` pattern or manual value tracking to get textarea value

**ChatInput.razor.css** — Scoped styles:
- `.chat-input-container` fixed at bottom, flex row, padding
- `textarea` dark background, light text, no resize, border-radius, auto-expand up to 200px max
- `.send-btn` circular button, accent color, disabled state styling
- Match existing dark theme

**ChatPanel.razor** — Main chat container with conversation state and streaming:
- Inject `ILLMService` and `IJSRuntime`
- Implement `IAsyncDisposable` for cleanup
- State: `List<ChatMessageModel>` for messages (simple class with Role, Content, IsStreaming properties)
- `_isGenerating` flag to disable input during streaming
- `CancellationTokenSource` for cancelling streams

- **SendMessage method:**
  1. Add user message to list
  2. Create assistant message with empty content, IsStreaming=true
  3. Set `_isGenerating = true`
  4. Build conversation history as `List<ChatMessageInput>` (from ILLMService types)
  5. Call `_llmService.StreamAsync(history, _cts.Token)` in `await foreach`
  6. Accumulate tokens in StringBuilder, update assistant message content
  7. Batch `StateHasChanged()` via `InvokeAsync` — every 50ms or 100 chars (use Stopwatch for timing)
  8. After stream completes: set IsStreaming=false, final StateHasChanged
  9. Set `_isGenerating = false`
  10. Handle OperationCanceledException gracefully

- **Auto-scroll:** After each batched StateHasChanged, check `chatHelpers.shouldAutoScroll("chat-messages")` and if true, call `chatHelpers.scrollToBottom("chat-messages")`

- **Empty state:** When no messages, show a centered welcome message like "Start a conversation" with a subtle icon

- **DisposeAsync:** Cancel CTS, dispose CTS

- Layout: scrollable message area (`id="chat-messages"`) taking remaining height, ChatInput fixed at bottom

**ChatPanel.razor.css** — Scoped styles:
- `.chat-panel` full height flex column
- `.chat-messages` flex-grow, overflow-y auto, scroll padding
- `.empty-state` centered, muted text
- Match dark theme

**Dashboard.razor** — Replace current summary-only dashboard:
- Keep existing summary cards at top
- Add `<ChatPanel />` component below the summary grid
- The chat panel should take remaining vertical space

**Dashboard.razor.css** — Update to accommodate chat panel:
- Adjust layout so chat panel fills remaining space below summary cards

**Define ChatMessageModel** as a simple class inside ChatPanel.razor @code block (or as a nested class):
```csharp
private class ChatMessageModel
{
    public string Role { get; set; } = "";
    public string Content { get; set; } = "";
    public bool IsStreaming { get; set; }
}
```

**IMPORTANT implementation notes:**
- Always use `await InvokeAsync(StateHasChanged)` — never bare `StateHasChanged()` from async streams
- Use `[EnumeratorCancellation]` is already on ILLMService.StreamAsync, so pass CancellationToken correctly
- The ChatMessageInput record is in `OpenAnima.Core.LLM` namespace — add `@using OpenAnima.Core.LLM` to ChatPanel
- Keyboard handling: Use `@onkeydown:preventDefault` conditionally or use JS interop to prevent Enter default
  </action>
  <verify>
    <automated>cd /home/user/OpenAnima && dotnet build src/OpenAnima.Core 2>&amp;1 | tail -5</automated>
    <manual>Run `dotnet run --project src/OpenAnima.Core --no-browser` and visit dashboard — chat panel should be visible below summary cards, typing a message and pressing Enter should trigger streaming response</manual>
  </verify>
  <done>Chat panel visible on Dashboard with message input. User can type and send messages. Assistant responses stream token-by-token. Messages show role-based styling (user right, assistant left). Auto-scroll works during streaming. Empty state shown when no messages.</done>
</task>

</tasks>

<verification>
1. `dotnet build src/OpenAnima.Core` compiles without errors
2. Dashboard page shows summary cards AND chat panel
3. User can type in the input box, press Enter to send
4. Shift+Enter creates a newline in the input
5. Input auto-expands as user types multiple lines
6. User message appears right-aligned with user background color
7. Assistant message appears left-aligned with AI icon and different background
8. Streaming tokens appear progressively (not all at once)
9. Chat auto-scrolls during streaming
10. If user scrolls up during streaming, auto-scroll pauses
11. Input is disabled during streaming
12. Empty state shows welcome message before first message
</verification>

<success_criteria>
- Chat panel renders on Dashboard with working input
- Messages display with correct role-based alignment and styling
- LLM responses stream token-by-token via ILLMService.StreamAsync
- Auto-scroll works with user-override detection
- Build passes with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-chat-ui-with-streaming/09-01-SUMMARY.md`
</output>

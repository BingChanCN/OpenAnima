---
phase: 17-e2e-module-pipeline-integration-editor-polish
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/OpenAnima.Core/Components/Shared/ChatPanel.razor
  - tests/OpenAnima.Tests/Integration/ModulePipelineIntegrationTests.cs
  - tests/OpenAnima.Tests/Integration/ChatPanelModulePipelineTests.cs
autonomous: true
requirements:
  - E2E-01
must_haves:
  truths:
    - "ChatPanel sends user messages through ChatInputModule instead of direct ILLMService calls"
    - "ChatPanel receives assistant messages from ChatOutputModule.OnMessageReceived through the wired module pipeline"
    - "When ChatInput→LLM→ChatOutput is not configured, ChatPanel shows guided prompt to open editor (no legacy direct API fallback)"
    - "With valid wiring, conversation behavior remains equivalent to v1.2 UX (user message + streaming-like assistant update flow)"
  artifacts:
    - path: "src/OpenAnima.Core/Components/Shared/ChatPanel.razor"
      provides: "Module-pipeline chat flow and pipeline-not-configured guided state"
      contains: "ChatInputModule.SendMessageAsync"
    - path: "tests/OpenAnima.Tests/Integration/ChatPanelModulePipelineTests.cs"
      provides: "Integration coverage for configured vs missing pipeline behavior"
      min_lines: 80
    - path: "tests/OpenAnima.Tests/Integration/ModulePipelineIntegrationTests.cs"
      provides: "Pipeline parity assertions for ChatInput→LLM→ChatOutput behavior"
  key_links:
    - from: "ChatPanel.razor"
      to: "ChatInputModule.SendMessageAsync"
      via: "SendMessage handler"
      pattern: "SendMessageAsync"
    - from: "ChatOutputModule.OnMessageReceived"
      to: "ChatPanel assistant message state"
      via: "event subscription + UI state update"
      pattern: "OnMessageReceived"
    - from: "ChatPanel pipeline check"
      to: "current wiring configuration"
      via: "runtime config validation before send"
      pattern: "ChatInputModule.*LLMModule.*ChatOutputModule"
---

<objective>
Route ChatPanel through the runtime module pipeline (ChatInput→LLM→ChatOutput) and remove direct LLM fallback behavior.

Purpose: Complete E2E-01 with real module wiring as the only execution path, while preserving v1.2 conversation UX when wiring is valid.
Output: ChatPanel pipeline integration + integration tests that prove configured and not-configured behaviors.
</objective>

<execution_context>
@/home/user/.claude/get-shit-done/workflows/execute-plan.md
@/home/user/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/home/user/OpenAnima/.planning/PROJECT.md
@/home/user/OpenAnima/.planning/ROADMAP.md
@/home/user/OpenAnima/.planning/STATE.md
@/home/user/OpenAnima/.planning/phases/17-e2e-module-pipeline-integration-editor-polish/17-CONTEXT.md
@/home/user/OpenAnima/.planning/phases/17-e2e-module-pipeline-integration-editor-polish/17-RESEARCH.md
@/home/user/OpenAnima/.planning/phases/16-module-runtime-initialization-port-registration/16-01-SUMMARY.md

<interfaces>
From src/OpenAnima.Core/Modules/ChatInputModule.cs:
```csharp
public class ChatInputModule : IModuleExecutor
{
    public Task SendMessageAsync(string message, CancellationToken ct = default);
}
```

From src/OpenAnima.Core/Modules/ChatOutputModule.cs:
```csharp
public class ChatOutputModule : IModuleExecutor
{
    public event Action<string>? OnMessageReceived;
    public string? LastReceivedText { get; }
}
```

From src/OpenAnima.Core/Components/Shared/ChatPanel.razor (current):
```csharp
@inject ILLMService _llmService   // remove usage for send/regenerate flow
private async Task SendMessage(string userMessage)
private async Task RegenerateLastResponse()
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace ChatPanel direct LLM path with module pipeline path</name>
  <files>src/OpenAnima.Core/Components/Shared/ChatPanel.razor</files>
  <action>Refactor ChatPanel so SendMessage and Regenerate use ChatInputModule + ChatOutputModule wiring flow, not _llmService.StreamWithUsageAsync. Implement locked decisions exactly: (1) no fallback to direct API path, (2) manual wiring required, (3) guided prompt with button/link to editor when required chain is not configured, and (4) dynamic validation against current wiring configuration (not hardcoded always-ready assumption). Keep existing context-limit checks and context manager updates; adapt response update to module output callback flow so user experience stays v1.2-like. Ensure event subscription lifecycle is safe (subscribe once, unsubscribe on dispose) and cancellation/regenerate behavior remains deterministic.</action>
  <verify>
    <automated>dotnet build /home/user/OpenAnima/src/OpenAnima.Core/OpenAnima.Core.csproj --no-restore</automated>
  </verify>
  <done>ChatPanel no longer depends on ILLMService streaming for normal send/regenerate, missing-pipeline state is user-guided, and configured pipeline path sends/receives through module contracts.</done>
</task>

<task type="auto">
  <name>Task 2: Add integration tests for ChatPanel pipeline configured and missing states</name>
  <files>
    tests/OpenAnima.Tests/Integration/ChatPanelModulePipelineTests.cs
    tests/OpenAnima.Tests/Integration/ModulePipelineIntegrationTests.cs
  </files>
  <action>Create/extend integration tests to prove E2E behavior: (A) with valid ChatInput→LLM→ChatOutput chain, sending from ChatPanel triggers ChatInputModule.SendMessageAsync and assistant output returns via ChatOutputModule.OnMessageReceived; (B) with missing chain, ChatPanel does not attempt legacy direct call and shows explicit guidance state pointing user to editor; (C) parity assertion that successful configured flow produces expected assistant text and stable message ordering. Keep tests under 60s runtime and use deterministic fake services where needed.</action>
  <verify>
    <automated>dotnet test /home/user/OpenAnima/tests/OpenAnima.Tests/OpenAnima.Tests.csproj --filter "FullyQualifiedName~ChatPanelModulePipelineTests|FullyQualifiedName~ModulePipelineIntegrationTests" --no-build</automated>
  </verify>
  <done>Automated tests fail if direct-path fallback reappears or if ChatPanel no longer works through module pipeline.</done>
</task>

</tasks>

<verification>
- ChatPanel module pipeline tests pass for both configured and not-configured states.
- No direct ILLMService streaming path remains for chat send/regenerate flow.
</verification>

<success_criteria>
- User can wire ChatInput→LLM→ChatOutput and chat through modules from ChatPanel.
- If chain is missing, user sees guided next step to editor instead of silent failure or legacy fallback.
</success_criteria>

<output>
After completion, create `/home/user/OpenAnima/.planning/phases/17-e2e-module-pipeline-integration-editor-polish/17-01-SUMMARY.md`
</output>

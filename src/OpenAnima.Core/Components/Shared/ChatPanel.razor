@using OpenAnima.Contracts
@using OpenAnima.Core.Events
@using OpenAnima.Core.LLM
@using OpenAnima.Core.Modules
@using OpenAnima.Core.Services
@using OpenAnima.Core.Wiring
@implements IAsyncDisposable
@inject ChatInputModule _chatInputModule
@inject ChatOutputModule _chatOutputModule
@inject ChatContextManager _contextManager
@inject IEventBus _eventBus
@inject IWiringEngine _wiringEngine
@inject NavigationManager _navigation
@inject IJSRuntime JS

<div class="chat-panel">
    @if (!IsPipelineConfigured())
    {
        <div class="pipeline-guidance" role="status">
            <p class="pipeline-guidance-title">Chat pipeline not configured</p>
            <p class="pipeline-guidance-body">Connect <code>ChatInputModule â†’ LLMModule â†’ ChatOutputModule</code> in the editor to enable conversation.</p>
            <button class="open-editor-btn" @onclick="OpenEditor">Open Wiring Editor</button>
        </div>
    }

    <div class="chat-messages" id="chat-messages">
        @if (_messages.Count == 0)
        {
            <div class="empty-state">
                <span class="empty-icon">ðŸ’¬</span>
                <p>Start a conversation</p>
            </div>
        }
        else
        {
            @foreach (var message in _messages)
            {
                <ChatMessage Role="@message.Role" Content="@message.Content" IsStreaming="@message.IsStreaming" />
            }
            @if (!_isGenerating && _messages.Count > 0 && _messages.Last().Role == "assistant")
            {
                <button class="regenerate-btn" @onclick="RegenerateLastResponse">
                    <span>ðŸ”„</span> Regenerate
                </button>
            }
        }
    </div>
    <TokenUsageDisplay
        InputTokens="@_contextManager.TotalInputTokens"
        OutputTokens="@_contextManager.TotalOutputTokens"
        CurrentContextTokens="@_contextManager.CurrentContextTokens"
        MaxContextTokens="@_contextManager.MaxContextTokens" />
    <ChatInput OnSend="SendMessage" IsDisabled="@(_isGenerating || IsContextLimitReached() || !IsPipelineConfigured())" />
</div>

@code {
    private readonly List<ChatMessageModel> _messages = new();
    private bool _isGenerating;
    private CancellationTokenSource _generationCts = new();
    private TaskCompletionSource<string>? _pendingAssistantResponse;

    protected override void OnInitialized()
    {
        _contextManager.OnStateChanged += HandleContextManagerChanged;
        _chatOutputModule.OnMessageReceived += HandleChatOutputReceived;
    }

    private void HandleContextManagerChanged()
    {
        InvokeAsync(StateHasChanged);
    }

    private void HandleChatOutputReceived(string assistantText)
    {
        _pendingAssistantResponse?.TrySetResult(assistantText);
    }

    private bool IsContextLimitReached()
    {
        var utilization = _contextManager.GetContextUtilization();
        return _contextManager.GetContextStatus() == ContextStatus.Danger && utilization >= 90.0;
    }

    private bool IsPipelineConfigured()
    {
        var currentConfig = _wiringEngine.GetCurrentConfiguration();
        return ChatPipelineConfigurationValidator.IsConfigured(currentConfig);
    }

    private async Task SendMessage(string userMessage)
    {
        if (string.IsNullOrWhiteSpace(userMessage) || _isGenerating)
            return;

        if (!IsPipelineConfigured())
        {
            await InvokeAsync(StateHasChanged);
            return;
        }

        var history = _messages
            .Where(m => !m.IsStreaming)
            .Select(m => new ChatMessageInput(m.Role, m.Content))
            .ToList();

        if (!_contextManager.CanSendMessage(history, userMessage))
        {
            var currentTokens = _contextManager.CurrentContextTokens;
            var maxTokens = _contextManager.MaxContextTokens;
            var utilization = _contextManager.GetContextUtilization();

            await JS.InvokeVoidAsync("chatHelpers.showContextLimitModal", currentTokens, maxTokens);
            await _eventBus.PublishAsync(new ModuleEvent<ContextLimitReachedPayload>
            {
                EventName = "ContextLimitReached",
                SourceModuleId = "OpenAnima.Core",
                Payload = new ContextLimitReachedPayload(currentTokens, maxTokens, utilization)
            });
            return;
        }

        _messages.Add(new ChatMessageModel
        {
            Role = "user",
            Content = userMessage,
            IsStreaming = false
        });

        var messageTokenCount = _contextManager.CountTokens(userMessage);
        _contextManager.UpdateAfterSend(messageTokenCount);
        await _eventBus.PublishAsync(new ModuleEvent<MessageSentPayload>
        {
            EventName = "MessageSent",
            SourceModuleId = "OpenAnima.Core",
            Payload = new MessageSentPayload(userMessage, messageTokenCount, DateTime.UtcNow)
        });

        var assistantMessage = new ChatMessageModel
        {
            Role = "assistant",
            Content = "",
            IsStreaming = true
        };
        _messages.Add(assistantMessage);

        await GenerateAssistantResponseAsync(userMessage, assistantMessage);
    }

    private async Task RegenerateLastResponse()
    {
        if (_isGenerating || !IsPipelineConfigured())
        {
            await InvokeAsync(StateHasChanged);
            return;
        }

        var lastAssistantIndex = _messages.FindLastIndex(m => m.Role == "assistant");
        if (lastAssistantIndex == -1)
            return;

        var lastUserMessage = _messages
            .Take(lastAssistantIndex)
            .LastOrDefault(m => m.Role == "user");

        if (lastUserMessage == null)
            return;

        _messages.RemoveAt(lastAssistantIndex);

        var assistantMessage = new ChatMessageModel
        {
            Role = "assistant",
            Content = "",
            IsStreaming = true
        };
        _messages.Add(assistantMessage);

        await GenerateAssistantResponseAsync(lastUserMessage.Content, assistantMessage);
    }

    private async Task GenerateAssistantResponseAsync(string prompt, ChatMessageModel assistantMessage)
    {
        ResetGenerationCancellation();

        _isGenerating = true;
        await InvokeAsync(StateHasChanged);

        using var timeoutCts = new CancellationTokenSource(TimeSpan.FromSeconds(30));
        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(_generationCts.Token, timeoutCts.Token);

        var responseTask = CreatePendingAssistantResponse(linkedCts.Token);

        try
        {
            await _chatInputModule.SendMessageAsync(prompt, linkedCts.Token);
            var assistantResponse = await responseTask;

            assistantMessage.Content = assistantResponse;
            assistantMessage.IsStreaming = false;

            await UpdateAfterResponseAsync(prompt, assistantResponse);
            await InvokeAsync(StateHasChanged);
            await ScrollToBottomIfNeededAsync();
        }
        catch (OperationCanceledException) when (timeoutCts.IsCancellationRequested && !_generationCts.IsCancellationRequested)
        {
            assistantMessage.Content = "No response arrived from the module pipeline. Open the editor and verify ChatInputModule â†’ LLMModule â†’ ChatOutputModule wiring.";
            assistantMessage.IsStreaming = false;
            await InvokeAsync(StateHasChanged);
        }
        catch (OperationCanceledException)
        {
            assistantMessage.Content += "\n\n[Cancelled]";
            assistantMessage.IsStreaming = false;
            await InvokeAsync(StateHasChanged);
        }
        finally
        {
            _pendingAssistantResponse = null;
            _isGenerating = false;
            await InvokeAsync(StateHasChanged);
        }
    }

    private Task<string> CreatePendingAssistantResponse(CancellationToken cancellationToken)
    {
        var completion = new TaskCompletionSource<string>(TaskCreationOptions.RunContinuationsAsynchronously);
        _pendingAssistantResponse = completion;

        if (cancellationToken.CanBeCanceled)
        {
            cancellationToken.Register(() => completion.TrySetCanceled(cancellationToken));
        }

        return completion.Task;
    }

    private async Task UpdateAfterResponseAsync(string prompt, string assistantResponse)
    {
        var inputTokens = _contextManager.CountTokens(prompt);
        var outputTokens = _contextManager.CountTokens(assistantResponse);
        _contextManager.UpdateAfterResponse(inputTokens, outputTokens, outputTokens);

        await _eventBus.PublishAsync(new ModuleEvent<ResponseReceivedPayload>
        {
            EventName = "ResponseReceived",
            SourceModuleId = "OpenAnima.Core",
            Payload = new ResponseReceivedPayload(
                assistantResponse,
                inputTokens,
                outputTokens,
                DateTime.UtcNow)
        });
    }

    private async Task ScrollToBottomIfNeededAsync()
    {
        var shouldScroll = await JS.InvokeAsync<bool>("chatHelpers.shouldAutoScroll", "chat-messages");
        if (shouldScroll)
        {
            await JS.InvokeVoidAsync("chatHelpers.scrollToBottom", "chat-messages");
        }
    }

    private void ResetGenerationCancellation()
    {
        _generationCts.Cancel();
        _generationCts.Dispose();
        _generationCts = new CancellationTokenSource();
    }

    private void OpenEditor()
    {
        _navigation.NavigateTo("/editor");
    }

    public ValueTask DisposeAsync()
    {
        _chatOutputModule.OnMessageReceived -= HandleChatOutputReceived;
        _contextManager.OnStateChanged -= HandleContextManagerChanged;

        _generationCts.Cancel();
        _generationCts.Dispose();

        return ValueTask.CompletedTask;
    }

    private class ChatMessageModel
    {
        public string Role { get; set; } = "";
        public string Content { get; set; } = "";
        public bool IsStreaming { get; set; }
    }
}

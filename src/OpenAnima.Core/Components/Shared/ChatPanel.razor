@using OpenAnima.Core.LLM
@using OpenAnima.Core.Services
@using OpenAnima.Core.Events
@using OpenAnima.Contracts
@using System.Text
@using System.Diagnostics
@implements IAsyncDisposable
@inject ILLMService _llmService
@inject ChatContextManager _contextManager
@inject IEventBus _eventBus
@inject IJSRuntime JS

<div class="chat-panel">
    <div class="chat-messages" id="chat-messages">
        @if (_messages.Count == 0)
        {
            <div class="empty-state">
                <span class="empty-icon">ðŸ’¬</span>
                <p>Start a conversation</p>
            </div>
        }
        else
        {
            @foreach (var message in _messages)
            {
                <ChatMessage Role="@message.Role" Content="@message.Content" IsStreaming="@message.IsStreaming" />
            }
            @if (!_isGenerating && _messages.Count > 0 && _messages.Last().Role == "assistant")
            {
                <button class="regenerate-btn" @onclick="RegenerateLastResponse">
                    <span>ðŸ”„</span> Regenerate
                </button>
            }
        }
    </div>
    <TokenUsageDisplay
        InputTokens="@_contextManager.TotalInputTokens"
        OutputTokens="@_contextManager.TotalOutputTokens"
        CurrentContextTokens="@_contextManager.CurrentContextTokens"
        MaxContextTokens="@_contextManager.MaxContextTokens" />
    <ChatInput OnSend="SendMessage" IsDisabled="@(_isGenerating || IsContextLimitReached())" />
</div>

@code {
    private List<ChatMessageModel> _messages = new();
    private bool _isGenerating = false;
    private CancellationTokenSource _cts = new();

    protected override void OnInitialized()
    {
        _contextManager.OnStateChanged += () => InvokeAsync(StateHasChanged);
    }

    private bool IsContextLimitReached()
    {
        var utilization = _contextManager.GetContextUtilization();
        return _contextManager.GetContextStatus() == ContextStatus.Danger && utilization >= 90.0;
    }

    private async Task SendMessage(string userMessage)
    {
        if (string.IsNullOrWhiteSpace(userMessage))
            return;

        // Build conversation history for context check
        var history = _messages
            .Where(m => !m.IsStreaming)
            .Select(m => new ChatMessageInput(m.Role, m.Content))
            .ToList();

        // Check context limit BEFORE sending
        if (!_contextManager.CanSendMessage(history, userMessage))
        {
            var currentTokens = _contextManager.CurrentContextTokens;
            var maxTokens = _contextManager.MaxContextTokens;
            var utilization = _contextManager.GetContextUtilization();

            await JS.InvokeVoidAsync("chatHelpers.showContextLimitModal", currentTokens, maxTokens);
            await _eventBus.PublishAsync(new ModuleEvent<ContextLimitReachedPayload>
            {
                EventName = "ContextLimitReached",
                SourceModuleId = "OpenAnima.Core",
                Payload = new ContextLimitReachedPayload(currentTokens, maxTokens, utilization)
            });
            return;
        }

        // Add user message
        _messages.Add(new ChatMessageModel
        {
            Role = "user",
            Content = userMessage,
            IsStreaming = false
        });

        // Update context manager after send
        var messageTokenCount = _contextManager.CountTokens(userMessage);
        _contextManager.UpdateAfterSend(messageTokenCount);
        await _eventBus.PublishAsync(new ModuleEvent<MessageSentPayload>
        {
            EventName = "MessageSent",
            SourceModuleId = "OpenAnima.Core",
            Payload = new MessageSentPayload(userMessage, messageTokenCount, DateTime.UtcNow)
        });

        // Create assistant message placeholder
        var assistantMessage = new ChatMessageModel
        {
            Role = "assistant",
            Content = "",
            IsStreaming = true
        };
        _messages.Add(assistantMessage);

        _isGenerating = true;
        await InvokeAsync(StateHasChanged);

        try
        {
            // Build conversation history
            history = _messages
                .Where(m => !m.IsStreaming)
                .Select(m => new ChatMessageInput(m.Role, m.Content))
                .ToList();

            // Stream response with usage tracking
            var contentBuilder = new StringBuilder();
            var stopwatch = Stopwatch.StartNew();
            var lastUpdateTime = 0L;
            const int updateIntervalMs = 50;
            const int updateIntervalChars = 100;
            var charsSinceLastUpdate = 0;
            int? finalInputTokens = null;
            int? finalOutputTokens = null;

            await foreach (var result in _llmService.StreamWithUsageAsync(history, _cts.Token))
            {
                contentBuilder.Append(result.Token);
                charsSinceLastUpdate += result.Token.Length;

                // Capture usage data when available
                if (result.InputTokens.HasValue)
                    finalInputTokens = result.InputTokens.Value;
                if (result.OutputTokens.HasValue)
                    finalOutputTokens = result.OutputTokens.Value;

                // Batch updates: every 50ms or 100 chars
                if (stopwatch.ElapsedMilliseconds - lastUpdateTime >= updateIntervalMs || charsSinceLastUpdate >= updateIntervalChars)
                {
                    assistantMessage.Content = contentBuilder.ToString();
                    await InvokeAsync(StateHasChanged);

                    // Auto-scroll if user hasn't scrolled up
                    var shouldScroll = await JS.InvokeAsync<bool>("chatHelpers.shouldAutoScroll", "chat-messages");
                    if (shouldScroll)
                    {
                        await JS.InvokeVoidAsync("chatHelpers.scrollToBottom", "chat-messages");
                    }

                    lastUpdateTime = stopwatch.ElapsedMilliseconds;
                    charsSinceLastUpdate = 0;
                }
            }

            // Final update
            assistantMessage.Content = contentBuilder.ToString();
            assistantMessage.IsStreaming = false;

            // Update context manager with API-returned usage
            if (finalInputTokens.HasValue && finalOutputTokens.HasValue)
            {
                var responseTokenCount = _contextManager.CountTokens(assistantMessage.Content);
                _contextManager.UpdateAfterResponse(finalInputTokens.Value, finalOutputTokens.Value, responseTokenCount);
                await _eventBus.PublishAsync(new ModuleEvent<ResponseReceivedPayload>
                {
                    EventName = "ResponseReceived",
                    SourceModuleId = "OpenAnima.Core",
                    Payload = new ResponseReceivedPayload(
                        assistantMessage.Content,
                        finalInputTokens.Value,
                        finalOutputTokens.Value,
                        DateTime.UtcNow)
                });
            }

            await InvokeAsync(StateHasChanged);

            // Final scroll
            var shouldScrollFinal = await JS.InvokeAsync<bool>("chatHelpers.shouldAutoScroll", "chat-messages");
            if (shouldScrollFinal)
            {
                await JS.InvokeVoidAsync("chatHelpers.scrollToBottom", "chat-messages");
            }
        }
        catch (OperationCanceledException)
        {
            // User cancelled - gracefully handle
            assistantMessage.Content += "\n\n[Cancelled]";
            assistantMessage.IsStreaming = false;
            await InvokeAsync(StateHasChanged);
        }
        finally
        {
            _isGenerating = false;
            await InvokeAsync(StateHasChanged);
        }
    }

    private async Task RegenerateLastResponse()
    {
        // Find and remove the last assistant message
        var lastAssistantIndex = _messages.FindLastIndex(m => m.Role == "assistant");
        if (lastAssistantIndex == -1)
            return;

        _messages.RemoveAt(lastAssistantIndex);

        // Create new assistant message placeholder
        var assistantMessage = new ChatMessageModel
        {
            Role = "assistant",
            Content = "",
            IsStreaming = true
        };
        _messages.Add(assistantMessage);

        _isGenerating = true;
        await InvokeAsync(StateHasChanged);

        try
        {
            // Build conversation history (without the removed assistant message)
            var history = _messages
                .Where(m => !m.IsStreaming)
                .Select(m => new ChatMessageInput(m.Role, m.Content))
                .ToList();

            // Stream response with usage tracking
            var contentBuilder = new StringBuilder();
            var stopwatch = Stopwatch.StartNew();
            var lastUpdateTime = 0L;
            const int updateIntervalMs = 50;
            const int updateIntervalChars = 100;
            var charsSinceLastUpdate = 0;
            int? finalInputTokens = null;
            int? finalOutputTokens = null;

            await foreach (var result in _llmService.StreamWithUsageAsync(history, _cts.Token))
            {
                contentBuilder.Append(result.Token);
                charsSinceLastUpdate += result.Token.Length;

                // Capture usage data when available
                if (result.InputTokens.HasValue)
                    finalInputTokens = result.InputTokens.Value;
                if (result.OutputTokens.HasValue)
                    finalOutputTokens = result.OutputTokens.Value;

                // Batch updates: every 50ms or 100 chars
                if (stopwatch.ElapsedMilliseconds - lastUpdateTime >= updateIntervalMs || charsSinceLastUpdate >= updateIntervalChars)
                {
                    assistantMessage.Content = contentBuilder.ToString();
                    await InvokeAsync(StateHasChanged);

                    // Auto-scroll if user hasn't scrolled up
                    var shouldScroll = await JS.InvokeAsync<bool>("chatHelpers.shouldAutoScroll", "chat-messages");
                    if (shouldScroll)
                    {
                        await JS.InvokeVoidAsync("chatHelpers.scrollToBottom", "chat-messages");
                    }

                    lastUpdateTime = stopwatch.ElapsedMilliseconds;
                    charsSinceLastUpdate = 0;
                }
            }

            // Final update
            assistantMessage.Content = contentBuilder.ToString();
            assistantMessage.IsStreaming = false;

            // Update context manager with API-returned usage
            if (finalInputTokens.HasValue && finalOutputTokens.HasValue)
            {
                var responseTokenCount = _contextManager.CountTokens(assistantMessage.Content);
                _contextManager.UpdateAfterResponse(finalInputTokens.Value, finalOutputTokens.Value, responseTokenCount);
                await _eventBus.PublishAsync(new ModuleEvent<ResponseReceivedPayload>
                {
                    EventName = "ResponseReceived",
                    SourceModuleId = "OpenAnima.Core",
                    Payload = new ResponseReceivedPayload(
                        assistantMessage.Content,
                        finalInputTokens.Value,
                        finalOutputTokens.Value,
                        DateTime.UtcNow)
                });
            }

            await InvokeAsync(StateHasChanged);

            // Final scroll
            var shouldScrollFinal = await JS.InvokeAsync<bool>("chatHelpers.shouldAutoScroll", "chat-messages");
            if (shouldScrollFinal)
            {
                await JS.InvokeVoidAsync("chatHelpers.scrollToBottom", "chat-messages");
            }
        }
        catch (OperationCanceledException)
        {
            // User cancelled - gracefully handle
            assistantMessage.Content += "\n\n[Cancelled]";
            assistantMessage.IsStreaming = false;
            await InvokeAsync(StateHasChanged);
        }
        finally
        {
            _isGenerating = false;
            await InvokeAsync(StateHasChanged);
        }
    }

    public async ValueTask DisposeAsync()
    {
        _cts.Cancel();
        _cts.Dispose();
    }

    private class ChatMessageModel
    {
        public string Role { get; set; } = "";
        public string Content { get; set; } = "";
        public bool IsStreaming { get; set; }
    }
}


